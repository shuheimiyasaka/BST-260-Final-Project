---
title: "Building Assocation and Prediction Models for the Loans Dataset"
output: html_notebook
---

```{r}
rm(list = ls())
library(tidyverse)
library(funModeling)
library(caret)
library(VIM)
library(mice)
library(ggcorrplot)
set.seed(1)
```

## Data Exploration

```{r}
setwd('/Users/shuheimiyasaka/Google Drive/Harvard/Courses/BST 260/BST-260-Final-Project')
load('loan.Rdata')
#loan.dat <- read.csv('loan.csv', header = TRUE)
#save(loan.dat, file = "./loan.RData")

#extract.3000 <- sample(1:dim(loan.dat)[1], 3000, replace = FALSE)
#write.csv(loan.dat[extract.3000, ], './loan_subset3000.csv')
```

```{r}
dim(loan.dat)
meta_loans <- funModeling::df_status(loan.dat, print_results = FALSE)
meta_loans
```

We have a data set with 887,379 records and 74 covariates.
From the above table, we notice quite a few missing data.

```{r}
cols.2.remove <- c('id', 'member_id', 'url', 'desc')
```

We immediately ruled out the following covariates in our model:
1) id
2) member_id
3) url
4) desc

```{r}
meta_loans[order(-meta_loans$p_na),]
missing.data.col <- meta_loans$variable[meta_loans$p_na > 10.]

cols.2.remove <- c(cols.2.remove, missing.data.col)
```

We also decided to remove records with more than 10% missing data.
```{r}
meta_loans[order(meta_loans$unique),]
cols.2.remove <- c(cols.2.remove, 'policy_code')
```
We also decided to remove `policy_code` sign it only has one unique value

```{r}
cols.2.keep <- !(colnames(loan.dat) %in% cols.2.remove)
colnames(loan.dat)[cols.2.keep]

loan.dat.subset <- loan.dat[, cols.2.keep]

meta_loans <- funModeling::df_status(loan.dat.subset, print_results = FALSE)
meta_loans
```

```{r}
numeric_cols <- meta_loans$variable[meta_loans$type == 'numeric']

ggcorrplot(cor(loan.dat.subset[,numeric_cols]))
#aggr(loan.dat.subset, combined=T, cex.axis=0.6)
```

```{r}
# loan.dat = mice(loan.dat.subset, m=1)  # let's just impute one dataset
# loan.dat = complete(loan.dat.subset, 1)
```
Can't impute since its too large.

```{r}
numeric_cols <- meta_loans$variable[meta_loans$type == 'numeric']
par(mfrow=c(7,7), mar=c(3,1,1.5,1))
for (j in 1:ncol(loan.dat.subset[numeric_cols])){
  hist(loan.dat.subset[,j], xlab="", main=names(loan.dat.subset)[j]) 
}

# Scatterplot of y vs each variable
par(mfrow=c(7,7),mar=c(3,1,1.5,1))
for (j in 1:ncol(loan.dat.subset[numeric_cols])){ 
  plot(D2[,j], y, xlab="", main=names(D2)[j], pch=20) 
}
```

```{r}
# keep a fifth of the data set to assess test performance
test.indx <- sample(1:dim(loan.dat.subset)[1], 200000, replace = FALSE)
train.indx <- setdiff(1:dim(loan.dat.subset)[1], test.indx)
```

We now onlhy have 50 convariates. after removing 24.

## Building a Prediction Model
```{r}


```
## Building an Assocation Model

## Conclusions

